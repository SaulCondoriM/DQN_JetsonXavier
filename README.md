<<<<<<< HEAD
# Sistema de Control Inteligente DQN Distribuido
## Informe T√©cnico - PC (Simulador) + Jetson Xavier (Agente de IA)

### Resumen Ejecutivo

Este proyecto implementa un **sistema de control inteligente distribuido** basado en **Deep Q-Network (DQN)** para el control aut√≥nomo de un robot m√≥vil diferencial. El sistema utiliza una arquitectura cliente-servidor donde:

- **PC (Cliente)**: Ejecuta la simulaci√≥n del entorno y el robot en Python con visualizaci√≥n en tiempo real
- **Jetson Xavier (Servidor)**: Ejecuta el agente de inteligencia artificial DQN en C++/CUDA para m√°ximo rendimiento

La comunicaci√≥n entre ambos sistemas se realiza mediante **protocolo TCP/IP**, permitiendo entrenamiento distribuido con separaci√≥n de responsabilidades: simulaci√≥n vs. procesamiento de IA.

### Arquitectura del Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         TCP/IP          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                PC (Cliente)             ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ         Jetson Xavier (Servidor)         ‚îÇ
‚îÇ                                         ‚îÇ                         ‚îÇ                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ   Estados (10 valores)  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ        Simulador 2D             ‚îÇ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  ‚îÇ  ‚îÇ         Servidor TCP            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ F√≠sica del robot             ‚îÇ   ‚îÇ        Formato CSV       ‚îÇ  ‚îÇ  ‚Ä¢ Parseo de estados            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Detecci√≥n de colisiones      ‚îÇ   ‚îÇ                         ‚îÇ  ‚îÇ  ‚Ä¢ Validaci√≥n de datos          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Sensores de distancia        ‚îÇ   ‚îÇ   Acciones (0-4)        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ C√°lculo de recompensas       ‚îÇ‚óÑ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ              ‚îÇ                          ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Gesti√≥n de episodios         ‚îÇ   ‚îÇ      (Enteros)          ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ                         ‚îÇ  ‚îÇ         DQN Agent               ‚îÇ   ‚îÇ
‚îÇ                                         ‚îÇ                         ‚îÇ  ‚îÇ  ‚Ä¢ Red Neuronal (CUDA)         ‚îÇ   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ                         ‚îÇ  ‚îÇ  ‚Ä¢ Replay Buffer                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ      Visualizaci√≥n GUI          ‚îÇ   ‚îÇ                         ‚îÇ  ‚îÇ  ‚Ä¢ Entrenamiento online         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Pygame 2D                    ‚îÇ   ‚îÇ                         ‚îÇ  ‚îÇ  ‚Ä¢ Target Network               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Robot, obst√°culos, goal      ‚îÇ   ‚îÇ                         ‚îÇ  ‚îÇ  ‚Ä¢ Pol√≠tica Œµ-greedy            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Trayectorias                 ‚îÇ   ‚îÇ                         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ                         ‚îÇ                                          ‚îÇ
‚îÇ                                         ‚îÇ                         ‚îÇ           GPU CUDA Cores                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Estructura del Proyecto y Descripci√≥n de Archivos

### Organizaci√≥n del C√≥digo

```
Final_DQN/
‚îú‚îÄ‚îÄ pc_simulator/           # üñ•Ô∏è  C√≥digo PC (Python) - Simulaci√≥n y Visualizaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ robot_simulator.py  # Simulador principal del robot diferencial
‚îÇ   ‚îú‚îÄ‚îÄ visualizer.py       # Interfaz gr√°fica con Pygame
‚îÇ   ‚îú‚îÄ‚îÄ tcp_client.py       # Cliente TCP de comunicaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ test_server.py      # Servidor de pruebas para desarrollo local
‚îÇ   ‚îî‚îÄ‚îÄ run_tests.py        # Suite de pruebas automatizadas
‚îÇ
‚îú‚îÄ‚îÄ jetson_agent/           # üöÄ C√≥digo Jetson (C++/CUDA) - Inteligencia Artificial  
‚îÇ   ‚îú‚îÄ‚îÄ include/            # Headers de las clases principales
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cuda_utils.cuh     # Utilidades y macros CUDA
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ neural_network.cuh # Implementaci√≥n de red neuronal DQN
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ replay_buffer.hpp  # Buffer de experiencias para entrenamiento
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dqn_agent.cuh      # Agente DQN completo con algoritmo
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tcp_server.hpp     # Servidor TCP robusto con manejo de errores
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.cu           # Programa principal y loop de entrenamiento
‚îÇ   ‚îî‚îÄ‚îÄ Makefile              # Compilaci√≥n optimizada para CUDA
‚îÇ
‚îî‚îÄ‚îÄ README.md                # Este documento
```

### Descripci√≥n Detallada de Archivos

#### üìÅ PC Simulator (Python)

**`robot_simulator.py`** - *N√∫cleo de la Simulaci√≥n*
- **Prop√≥sito**: Simula un robot diferencial en entorno 2D con f√≠sica realista
- **Funcionalidades**:
  - Modelo din√°mico del robot (velocidad lineal/angular)
  - Sistema de sensores de distancia (3 sensores: frontal, izquierdo, derecho)
  - Detecci√≥n de colisiones con obst√°culos y l√≠mites del entorno
  - C√°lculo de recompensas basado en progreso hacia objetivo
  - Gesti√≥n de episodios (reset, terminaci√≥n)
- **Entorno**: 3√ó3 metros con 2 obst√°culos estrat√©gicos
- **Estados**: Vector de 10 dimensiones (posici√≥n, orientaci√≥n, velocidades, sensores, objetivo)

**`visualizer.py`** - *Interfaz Gr√°fica*
- **Prop√≥sito**: Visualizaci√≥n en tiempo real del entrenamiento con Pygame
- **Elementos visuales**:
  - Robot (c√≠rculo azul con orientaci√≥n)
  - Obst√°culos (c√≠rculos rojos)
  - Objetivo (c√≠rculo verde)
  - Trayectoria del robot
  - Informaci√≥n de estado (episodio, recompensa, pasos)
- **Modos**: Visualizaci√≥n continua o por intervalos para optimizaci√≥n

**`tcp_client.py`** - *Comunicaci√≥n TCP*
- **Prop√≥sito**: Cliente TCP que conecta con el agente DQN en Jetson
- **Protocolo**: Env√≠o s√≠ncrono de estados y recepci√≥n de acciones
- **Manejo de errores**: Reconexi√≥n autom√°tica y sincronizaci√≥n robusta
- **Formato de datos**: CSV para estados, enteros para acciones

#### üîß Jetson Agent (C++/CUDA)

**`neural_network.cuh`** - *Red Neuronal DQN*
- **Prop√≥sito**: Implementaci√≥n de red neuronal profunda en CUDA
- **Arquitectura**: 
  ```
  Input(10) ‚Üí Dense(128,ReLU) ‚Üí Dense(128,ReLU) ‚Üí Dense(64,ReLU) ‚Üí Output(5)
  ```
- **Optimizaci√≥n**: Kernels CUDA personalizados para forward/backward pass
- **Memoria**: Gesti√≥n eficiente de memoria GPU
- **Target Network**: Implementaci√≥n de red objetivo para estabilidad

**`replay_buffer.hpp`** - *Buffer de Experiencias*
- **Prop√≥sito**: Almacenar experiencias (s,a,r,s') para entrenamiento por lotes
- **Capacidad**: 100,000 experiencias con sobrescritura circular
- **Muestreo**: Selecci√≥n aleatoria de mini-lotes de 64 experiencias
- **Optimizaci√≥n**: Acceso eficiente a memoria para entrenamiento continuo

**`dqn_agent.cuh`** - *Agente DQN Principal*
- **Prop√≥sito**: Implementa el algoritmo completo de Deep Q-Network
- **Caracter√≠sticas**:
  - Pol√≠tica Œµ-greedy con decaimiento (1.0 ‚Üí 0.05)
  - Entrenamiento online cada paso
  - Soft update de target network (œÑ = 0.005)
  - Guardado/carga de modelos entrenados
- **Hiperpar√°metros optimizados**: lr=0.0005, Œ≥=0.99, batch=64

**`tcp_server.hpp`** - *Servidor TCP Robusto*
- **Prop√≥sito**: Servidor TCP que recibe estados y env√≠a acciones
- **Robustez**: Parseo seguro con manejo de errores de formato
- **Funciones de seguridad**: `safe_stof()`, `safe_stoi()` para evitar excepciones
- **Gesti√≥n de conexiones**: Manejo de desconexiones y reconexiones

**`main.cu`** - *Programa Principal*
- **Prop√≥sito**: Orquestaci√≥n del entrenamiento completo
- **Loop principal**:
  1. Inicializaci√≥n de CUDA y red neuronal
  2. Creaci√≥n de servidor TCP en puerto 5555
  3. Ciclo de entrenamiento epis√≥dico
  4. Guardado peri√≥dico de modelos
- **M√©tricas**: Logging de recompensas, epsilon, loss durante entrenamiento

## Protocolo de Comunicaci√≥n TCP/IP

### Arquitectura de Red

El sistema utiliza un **protocolo TCP personalizado** para comunicaci√≥n distribuida:

- **Jetson Xavier**: Act√∫a como **SERVIDOR** (puerto 5555)
- **PC**: Act√∫a como **CLIENTE** (conecta al Jetson)
- **Protocolo**: S√≠ncrono, un mensaje por paso de simulaci√≥n

### Datos Enviados: PC ‚Üí Jetson (Estado del Robot)

El PC env√≠a el estado completo del robot en **formato CSV** con 10 valores:

```csv
x,y,theta,v,omega,d_front,d_left,d_right,dx_goal,dy_goal|done,reward,goal,collision
```

| Campo | Tipo | Rango | Descripci√≥n |
|-------|------|-------|-------------|
| `x` | float | [0.0, 3.0] | Posici√≥n X del robot (metros) |
| `y` | float | [0.0, 3.0] | Posici√≥n Y del robot (metros) |
| `theta` | float | [-œÄ, œÄ] | Orientaci√≥n del robot (radianes) |
| `v` | float | [0.0, 2.0] | Velocidad lineal actual (m/s) |
| `omega` | float | [-2.0, 2.0] | Velocidad angular actual (rad/s) |
| `d_front` | float | [0.0, 5.0] | Distancia sensor frontal (metros) |
| `d_left` | float | [0.0, 5.0] | Distancia sensor izquierdo (metros) |
| `d_right` | float | [0.0, 5.0] | Distancia sensor derecho (metros) |
| `dx_goal` | float | [-3.0, 3.0] | Componente X hacia objetivo |
| `dy_goal` | float | [-3.0, 3.0] | Componente Y hacia objetivo |

**Ejemplo de mensaje real:**
```
1.5,0.8,0.785,1.2,-0.3,2.1,1.8,3.2,-1.0,1.7|0,-0.1,0,0
```

### Datos Recibidos: Jetson ‚Üí PC (Acci√≥n Seleccionada)

El Jetson responde con un **entero** que representa la acci√≥n a ejecutar:

| Acci√≥n | Valor | Efecto en el Robot |
|--------|-------|-------------------|
| **FORWARD** | `0` | Acelerar hacia adelante (v += 0.5) |
| **LEFT** | `1` | Girar a la izquierda (œâ += 1.0) |
| **RIGHT** | `2` | Girar a la derecha (œâ -= 1.0) |
| **BRAKE** | `3` | Frenar (v *= 0.5, œâ *= 0.5) |
| **BACKWARD** | `4` | Retroceder (v -= 0.3) |

### Procesamiento en Jetson Xavier

#### 1. **Recepci√≥n y Parseo de Estados**
```cpp
// tcp_server.hpp - Parseo robusto
std::vector<float> parseState(const std::string& message) {
    // Separar por comas y convertir con safe_stof()
    // Validar rangos y detectar errores de formato
    // Retornar vector normalizado para la red neuronal
}
```

#### 2. **Inferencia DQN (Forward Pass)**
```cpp
// neural_network.cuh - Procesamiento CUDA
__global__ void forward_pass_kernel(float* input, float* output, 
                                   float* weights, int batch_size) {
    // 1. Propagaci√≥n hacia adelante en GPU
    // 2. Activaciones ReLU entre capas
    // 3. C√°lculo de Q-values para las 5 acciones
}
```

**Flujo de procesamiento:**
1. **Normalizaci√≥n**: Estados se normalizan a rango [0,1]
2. **GPU Transfer**: Datos se copian a memoria GPU
3. **Forward Pass**: Red neuronal procesa entrada ‚Üí Q-values
4. **Selecci√≥n de Acci√≥n**: Œµ-greedy sobre Q-values m√°ximos
5. **CPU Return**: Acci√≥n seleccionada regresa a CPU

#### 3. **Entrenamiento DQN (Backward Pass)**
```cpp
// dqn_agent.cuh - Entrenamiento online
void train_step() {
    // 1. Muestrear mini-lote del replay buffer (64 experiencias)
    // 2. Calcular Q-targets usando target network
    // 3. Forward pass en main network
    // 4. Calcular loss MSE: (Q_pred - Q_target)¬≤
    // 5. Backward pass y actualizaci√≥n de pesos (Adam optimizer)
    // 6. Soft update de target network
}
```

### Gesti√≥n de Errores y Robustez

#### En el PC (Cliente):
- **Reconexi√≥n autom√°tica** si se pierde conexi√≥n
- **Timeout** de 5 segundos por mensaje
- **Validaci√≥n** de respuestas del Jetson
- **Sincronizaci√≥n** robusta entre episodios

#### En el Jetson (Servidor):
- **Parseo seguro** con funciones `safe_stof()`
- **Validaci√≥n de rangos** de los estados recibidos
- **Manejo de clientes m√∫ltiples** (aunque solo uno activo)
- **Recovery** autom√°tico de errores de formato

## Algoritmo Deep Q-Network (DQN)

### Fundamentos Te√≥ricos

El **Deep Q-Network** es un algoritmo de aprendizaje por refuerzo que combina:
- **Q-Learning**: Algoritmo de diferencias temporales para estimar valores Q(s,a)
- **Redes Neuronales Profundas**: Aproximaci√≥n de funciones para espacios de estados continuos
- **Experience Replay**: Buffer de experiencias para entrenamiento estable
- **Target Network**: Red objetivo para estabilizar el entrenamiento

### Implementaci√≥n en CUDA

#### **Arquitectura de Red Neuronal**
```
Entrada (10 dimensiones)
    ‚Üì
Capa Densa 1: 10 ‚Üí 128 neuronas + ReLU
    ‚Üì  
Capa Densa 2: 128 ‚Üí 128 neuronas + ReLU
    ‚Üì
Capa Densa 3: 128 ‚Üí 64 neuronas + ReLU  
    ‚Üì
Salida: 64 ‚Üí 5 Q-values (una por acci√≥n)
```

**Par√°metros totales**: ~35,000 pesos entrenables

#### **Funci√≥n de P√©rdida (Loss Function)**
```
L(Œ∏) = E[(Q_target - Q_pred)¬≤]

Donde:
Q_target = r + Œ≥ * max_a' Q_target(s', a')
Q_pred = Q_main(s, a)
```

#### **Hiperpar√°metros Optimizados**

| Par√°metro | Valor | Justificaci√≥n |
|-----------|-------|---------------|
| **Learning Rate** | 0.0005 | Convergencia estable sin overshooting |
| **Gamma (Œ≥)** | 0.99 | Prioriza recompensas futuras (visi√≥n a largo plazo) |
| **Epsilon inicial** | 1.0 | Exploraci√≥n m√°xima al inicio |
| **Epsilon final** | 0.05 | Mantiene 5% exploraci√≥n para adaptabilidad |
| **Epsilon decay** | 0.9999 | Decaimiento gradual (5000 episodios) |
| **Batch Size** | 64 | Balance entre estabilidad y eficiencia GPU |
| **Replay Buffer** | 100,000 | Suficiente diversidad sin consumir memoria |
| **Tau (œÑ)** | 0.005 | Soft update lento para estabilidad |
| **Train Frequency** | 1 | Entrenamiento en cada paso (online) |

### Funci√≥n de Recompensa Dise√±ada

La funci√≥n de recompensa est√° **cuidadosamente dise√±ada** para guiar el aprendizaje:

```python
def calculate_reward(self):
    reward = 0.0
    
    # üéØ OBJETIVO PRINCIPAL
    if self.check_goal_reached():
        return +100.0  # Recompensa m√°xima por √©xito
    
    # ‚ö†Ô∏è PENALIZACI√ìN POR COLISI√ìN  
    if self.check_collision():
        return -100.0  # Penalizaci√≥n m√°xima por fallo
    
    # üìà PROGRESO HACIA EL OBJETIVO
    dist_atual = np.linalg.norm(self.position - self.goal)
    if dist_atual < self.prev_distance:
        reward += 10.0 * (self.prev_distance - dist_atual)  # Recompensa por acercarse
    
    # ‚è±Ô∏è COSTO POR TIEMPO
    reward -= 0.1  # Incentiva soluciones r√°pidas
    
    # üö´ PENALIZACI√ìN POR ACCIONES INEFICIENTES
    if action == BRAKE:
        reward -= 0.2  # Desincentivar frenado excesivo
    elif action == BACKWARD:
        reward -= 0.15  # Desincentivar retroceso
    
    # üéØ BONIFICACI√ìN POR PROXIMIDAD AL OBJETIVO
    if dist_atual < 1.0:
        reward += 0.5  # Cerca del objetivo
    elif dist_atual < 2.0:
        reward += 0.2  # Moderadamente cerca
        
    # ‚ö†Ô∏è PENALIZACI√ìN POR PROXIMIDAD A OBST√ÅCULOS
    if min(d_front, d_left, d_right) < 0.3:
        reward -= 0.5  # Incentiva mantener distancia segura
        
    return reward
```

### Proceso de Entrenamiento

#### **Ciclo de Entrenamiento por Episodio:**

1. **Inicializaci√≥n**
   - Robot se posiciona en (0.1, 0.1)
   - Objetivo en (2.5, 2.5) 
   - Obst√°culos fijos en posiciones estrat√©gicas

2. **Loop de Pasos** (m√°ximo 150 pasos por episodio)
   ```cpp
   for (int step = 0; step < max_steps; step++) {
       // 1. Recibir estado del PC
       state = tcp_server.receive_state();
       
       // 2. Seleccionar acci√≥n (Œµ-greedy)
       action = agent.select_action(state, epsilon);
       
       // 3. Enviar acci√≥n al PC
       tcp_server.send_action(action);
       
       // 4. Almacenar experiencia
       replay_buffer.add(prev_state, action, reward, state, done);
       
       // 5. Entrenar si hay suficientes experiencias
       if (replay_buffer.size() >= min_replay_size) {
           agent.train_step();
       }
   }
   ```

3. **Actualizaci√≥n de Par√°metros**
   - Decaimiento de epsilon: `Œµ = Œµ * decay_rate`
   - Soft update de target network: `Œ∏_target = œÑ*Œ∏_main + (1-œÑ)*Œ∏_target`
   - Guardado de modelo cada 100 episodios

#### **M√©tricas de Entrenamiento Monitoreadas:**

- **Recompensa acumulada por episodio**
- **Tasa de √©xito** (episodios que alcanzan objetivo)
- **N√∫mero de pasos promedio** hasta completar tarea
- **Loss de la red neuronal** (MSE)
- **Valor de epsilon actual** (exploraci√≥n vs explotaci√≥n)

## Configuraci√≥n de Red y Ejecuci√≥n

### Configuraci√≥n de Red Distribuida

#### **Paso 1: Configurar IPs de los Dispositivos**

**En el Jetson Xavier (Servidor):**
```bash
# Obtener IP del Jetson
ip addr show eth0        # Ethernet (recomendado para estabilidad)
# O para WiFi:
ip addr show wlan0

# Ejemplo de salida: inet 192.168.18.114/24
```

**En el PC (Cliente):**
```bash
# Verificar conectividad con el Jetson
ping 192.168.18.114     # Usar la IP real del Jetson

# Opcional: Verificar puerto abierto
nc -zv 192.168.18.114 5555
```

#### **Paso 2: Configurar Firewall (si es necesario)**

**En el Jetson Xavier:**
```bash
# Permitir puerto 5555 para comunicaci√≥n TCP
sudo ufw allow 5555/tcp

# O desactivar firewall temporalmente durante desarrollo
sudo ufw disable
```

### Instalaci√≥n y Compilaci√≥n

#### **En el PC (Python 3.13+)**
```bash
# Instalar dependencias del simulador
pip3 install numpy pygame

# Verificar instalaci√≥n
cd pc_simulator
python3 -c "import numpy, pygame; print('Dependencias OK')"

# Ejecutar pruebas del simulador
python3 run_tests.py --test simulator
```

#### **En el Jetson Xavier (CUDA 12.2+)**
```bash
# Transferir c√≥digo al Jetson
scp -r jetson_agent/ usuario@192.168.18.114:~/Final_DQN/

# Conectar al Jetson y compilar
ssh usuario@192.168.18.114
cd ~/Final_DQN/jetson_agent

# Verificar CUDA disponible
nvidia-smi
nvcc --version

# Compilar el agente DQN
make clean && make

# Verificar compilaci√≥n exitosa
ls -la bin/dqn_agent    # Debe existir el ejecutable
```

### Ejecuci√≥n del Sistema Distribuido

#### **Secuencia de Inicio (IMPORTANTE: Orden espec√≠fico)**

**1. Iniciar Servidor DQN en Jetson (PRIMERO):**
```bash
# En terminal del Jetson Xavier
cd ~/Final_DQN/jetson_agent
./bin/dqn_agent --port 5555 --episodes 500

# Salida esperada:
# [CUDA] Inicializando dispositivo GPU...
# [DQN] Creando red neuronal 10->128->128->64->5
# [TCP] Servidor esperando conexiones en puerto 5555...
```

**2. Iniciar Cliente Simulador en PC (SEGUNDO):**
```bash
# En terminal del PC
cd /home/saul/Documentos/Final_DQN/pc_simulator
python3 tcp_client.py --ip 192.168.18.114 --port 5555 --episodes 500 --visualize

# Salida esperada:
# [TCP] Conectando a 192.168.18.114:5555...
# [SIMULATOR] Iniciando entrenamiento DQN...
# [EPISODE 1] Recompensa: -45.2, Pasos: 150, Epsilon: 0.998
```

### Par√°metros de Configuraci√≥n

#### **Opciones del Simulador (PC)**
| Par√°metro | Descripci√≥n | Valor Default | Rango |
|-----------|-------------|---------------|-------|
| `--ip` | IP del Jetson Xavier | `127.0.0.1` | IP v√°lida |
| `--port` | Puerto TCP de comunicaci√≥n | `5555` | 1024-65535 |
| `--episodes` | N√∫mero total de episodios | `1000` | 1-‚àû |
| `--visualize` | Mostrar GUI en tiempo real | `False` | True/False |
| `--render-every` | Renderizar cada N episodios | `10` | 1-100 |
| `--save-logs` | Guardar m√©tricas en archivo | `True` | True/False |

#### **Opciones del Agente DQN (Jetson)**
| Par√°metro | Descripci√≥n | Valor Default | Rango |
|-----------|-------------|---------------|-------|
| `--port` | Puerto TCP del servidor | `5555` | 1024-65535 |
| `--episodes` | Episodios m√°ximos (-1=‚àû) | `-1` | -1,1-‚àû |
| `--model-path` | Ruta del modelo DQN | `models/dqn_model.bin` | Path v√°lido |
| `--load-model` | Cargar modelo existente | `False` | True/False |
| `--save-every` | Guardar modelo cada N episodios | `100` | 10-1000 |
| `--device-id` | ID del dispositivo CUDA | `0` | 0-N |

### Monitoreo del Entrenamiento

#### **En el Jetson (Logs de IA):**
```
[EPISODE 0001] Reward: -85.4  | Loss: 2.45  | Epsilon: 0.999 | Steps: 150
[EPISODE 0050] Reward: -12.3  | Loss: 0.87  | Epsilon: 0.951 | Steps: 89
[EPISODE 0100] Reward: +45.7  | Loss: 0.34  | Epsilon: 0.905 | Steps: 67
[EPISODE 0200] Reward: +89.2  | Loss: 0.18  | Epsilon: 0.819 | Steps: 34
```

#### **En el PC (Logs de Simulaci√≥n):**
```
[SIM] Episodio 1/500 | Goal: NO | Colisi√≥n: S√ç | Pasos: 150 | R_total: -85.4
[SIM] Episodio 50/500 | Goal: NO | Colisi√≥n: S√ç | Pasos: 89 | R_total: -12.3  
[SIM] Episodio 100/500 | Goal: S√ç | Colisi√≥n: NO | Pasos: 67 | R_total: +45.7
[SIM] Episodio 200/500 | Goal: S√ç | Colisi√≥n: NO | Pasos: 34 | R_total: +89.2
```

## Resultados Esperados y An√°lisis de Rendimiento

### Curva de Aprendizaje Esperada

El entrenamiento del DQN sigue un patr√≥n caracter√≠stico dividido en **4 fases**:

#### **Fase 1: Exploraci√≥n Inicial (Episodios 1-50)**
- **Recompensa promedio**: -80 a -50
- **Tasa de √©xito**: 0-5%
- **Comportamiento**: Movimientos aleatorios, muchas colisiones
- **Epsilon**: 1.0 ‚Üí 0.95 (95% exploraci√≥n)

#### **Fase 2: Aprendizaje B√°sico (Episodios 51-150)**  
- **Recompensa promedio**: -50 a -10
- **Tasa de √©xito**: 5-25%
- **Comportamiento**: Comienza a evitar obst√°culos, movimientos m√°s dirigidos
- **Epsilon**: 0.95 ‚Üí 0.86 (86% exploraci√≥n)

#### **Fase 3: Refinamiento (Episodios 151-350)**
- **Recompensa promedio**: -10 a +60
- **Tasa de √©xito**: 25-70%
- **Comportamiento**: Encuentra rutas v√°lidas consistentemente
- **Epsilon**: 0.86 ‚Üí 0.70 (70% exploraci√≥n)

#### **Fase 4: Convergencia (Episodios 351-500+)**
- **Recompensa promedio**: +60 a +95
- **Tasa de √©xito**: 70-95%
- **Comportamiento**: Pol√≠tica casi √≥ptima, rutas eficientes
- **Epsilon**: 0.70 ‚Üí 0.05 (5% exploraci√≥n residual)

### M√©tricas de Evaluaci√≥n

#### **M√©tricas Primarias:**
- **Tasa de √âxito**: % de episodios que alcanzan el objetivo
- **Recompensa Acumulada**: Suma de recompensas por episodio
- **Pasos hasta Objetivo**: Eficiencia de las rutas encontradas
- **Tiempo de Convergencia**: Episodios necesarios para pol√≠tica estable

#### **M√©tricas T√©cnicas:**
- **Loss de Red Neuronal**: Error MSE entre Q_pred y Q_target
- **Utilizaci√≥n de GPU**: % de uso de CUDA cores durante entrenamiento  
- **Throughput**: Pasos procesados por segundo
- **Memoria GPU**: Uso de VRAM para redes y replay buffer

## Pruebas y Validaci√≥n

### Configuraci√≥n del Entorno de Prueba

#### **Entorno Optimizado para Aprendizaje R√°pido:**
- **Dimensiones**: 3√ó3 metros (reducido para acelerar convergencia)
- **Posici√≥n inicial robot**: (0.1, 0.1) 
- **Objetivo**: (2.5, 2.5)
- **Obst√°culos**: 2 obst√°culos estrat√©gicamente ubicados
  - Obst√°culo 1: Centro (1.5, 1.5), radio 0.35m - bloquea ruta directa
  - Obst√°culo 2: (2.2, 1.0), radio 0.2m - bloquea diagonal inferior
- **Pasos m√°ximos**: 150 por episodio

#### **Validaci√≥n de Dificultad del Entorno:**
Antes del entrenamiento DQN, se valid√≥ que el entorno requiere aprendizaje:

```bash
# Prueba con pol√≠tica directa (sin evasi√≥n)
=== Test: Pol√≠tica DIRECTA (sin evasi√≥n) ===
Resultado: 0/10 goals, 10/10 colisiones

# Prueba con pol√≠tica aleatoria  
=== Test: Pol√≠tica ALEATORIA ===
Resultado: 0/10 goals, 10/10 colisiones

>> Un DQN entrenado deber√≠a superar ambas pol√≠ticas!
```

**Conclusi√≥n**: Ambiente desafiante que requiere aprendizaje para tener √©xito.

### Pruebas Locales de Desarrollo

Para desarrollo y debugging sin el Jetson, usa el servidor de prueba:

#### **Servidor de Prueba en Python:**
```bash
# Terminal 1: Servidor de prueba (simula Jetson)
cd pc_simulator  
python3 test_server.py --port 5555 --episodes 100 --policy random

# Terminal 2: Cliente simulador
cd pc_simulator
python3 tcp_client.py --ip 127.0.0.1 --port 5555 --episodes 100 --visualize
```

#### **Pol√≠ticas de Prueba Disponibles:**
| Pol√≠tica | Descripci√≥n | Uso |
|----------|-------------|-----|
| `random` | Acciones aleatorias uniformes | Baseline inferior |
| `forward` | Solo avanzar (sin evasi√≥n) | Validar obst√°culos |
| `simple` | Giros simples al detectar obst√°culos | Heur√≠stica b√°sica |
| `dqn` | Cargar modelo DQN entrenado | Validar agente |

### Suite de Pruebas Automatizadas

#### **Ejecutar todas las pruebas:**
```bash
cd pc_simulator
python3 run_tests.py --all

# O pruebas espec√≠ficas:
python3 run_tests.py --test simulator      # Prueba simulador solo  
python3 run_tests.py --test tcp           # Prueba comunicaci√≥n TCP
python3 run_tests.py --test environment   # Prueba configuraci√≥n entorno
```

#### **Pruebas de Rendimiento:**
```bash
# Benchmark de throughput del simulador
python3 run_tests.py --test performance --episodes 1000

# Salida esperada:
# [PERF] Simulaci√≥n: 2847 pasos/segundo
# [PERF] TCP: 1923 mensajes/segundo  
# [PERF] Renderizaci√≥n: 60 FPS promedio
```

## Soluci√≥n de Problemas y Debugging

### Problemas Comunes de Conectividad

#### **Error: "Connection refused" o "No route to host"**
```bash
# 1. Verificar que el Jetson est√© ejecutando el servidor
ssh usuario@192.168.18.114
ps aux | grep dqn_agent    # Debe aparecer el proceso

# 2. Verificar IP correcta del Jetson
ip addr show eth0          # Confirmar IP real

# 3. Probar conectividad b√°sica
ping 192.168.18.114        # Desde el PC
nc -zv 192.168.18.114 5555 # Probar puerto espec√≠fico

# 4. Configurar firewall en Jetson
sudo ufw allow 5555/tcp
# O temporalmente: sudo ufw disable
```

#### **Error: "CUDA out of memory"**
```bash
# 1. Verificar memoria GPU disponible
nvidia-smi

# 2. Si hay otros procesos usando GPU, terminarlos
sudo fuser -v /dev/nvidia*
sudo kill -9 <PID_DEL_PROCESO>

# 3. Reducir batch size en main.cu si es necesario
# Cambiar BATCH_SIZE de 64 a 32 o 16
```

#### **Error: "stof exception" o parseo de datos**
```bash
# Verificar formato de mensajes TCP
# En pc_simulator/tcp_client.py, a√±adir debug:
print(f"Enviando: {message}")

# En jetson_agent/include/tcp_server.hpp
# Ya tiene manejo robusto con safe_stof()
```

### Problemas de Entrenamiento

#### **El agente no aprende (recompensa no mejora)**
1. **Verificar replay buffer**: Debe acumular al menos 500 experiencias
2. **Ajustar epsilon decay**: Muy r√°pido impide exploraci√≥n
3. **Revisar funci√≥n de recompensa**: Debe dar feedback √∫til
4. **Aumentar episodios**: DQN necesita 200-500 episodios m√≠nimo

#### **Convergencia muy lenta**  
1. **Reducir tama√±o del entorno**: Ya optimizado a 3√ó3m
2. **Ajustar learning rate**: Probar 0.001 si 0.0005 es muy lento
3. **Simplificar obst√°culos**: Reducir de 2 a 1 obst√°culo temporalmente

#### **Inestabilidad en el entrenamiento**
1. **Verificar target network**: Debe actualizarse cada 100 pasos
2. **Revisar soft update tau**: 0.005 es conservativo y estable
3. **Monitorear loss**: No debe crecer indefinidamente

### Debugging Avanzado

#### **Logs Detallados en Jetson:**
```cpp
// En main.cu, a√±adir:
#define DEBUG_MODE 1

// Habilita logs extendidos:
// [DEBUG] Estado recibido: [1.2, 0.8, 0.785, ...]
// [DEBUG] Q-values: [0.23, -0.45, 0.78, -0.12, 0.34]
// [DEBUG] Acci√≥n seleccionada: 2 (epsilon=0.891)
```

#### **Profiling de Rendimiento:**
```bash
# En Jetson, usar nvprof para an√°lisis CUDA
nvprof --log-file dqn_profile.txt ./bin/dqn_agent --episodes 10

# Analizar cuellos de botella:
# GPU utilization, memory transfers, kernel execution time
```

#### **Visualizaci√≥n de M√©tricas:**
```python
# En PC, modificar tcp_client.py para guardar m√©tricas
import matplotlib.pyplot as plt

rewards = []  # Recolectar durante entrenamiento
plt.plot(rewards)
plt.xlabel('Episodio')
plt.ylabel('Recompensa Acumulada')  
plt.title('Curva de Aprendizaje DQN')
plt.show()
```

## Notas T√©cnicas sobre Hardware

### Requisitos del Sistema

#### **PC (Cliente - Simulador):**
- **CPU**: Intel/AMD multi-core (4+ cores recomendado)
- **RAM**: 4 GB m√≠nimo, 8 GB recomendado  
- **Python**: 3.8+ (probado con Python 3.13)
- **Dependencias**: NumPy, Pygame
- **Red**: Conexi√≥n Ethernet/WiFi estable con Jetson

#### **Jetson Xavier (Servidor - Agente IA):**
- **GPU**: 512 CUDA cores (Volta), 32 Tensor cores
- **RAM**: 16/32 GB (probado con 32 GB)
- **CUDA**: Compute Capability 7.2 (sm_72)
- **Storage**: 10 GB disponible para modelos y logs
- **Red**: Ethernet preferido para estabilidad

### Configuraci√≥n CUDA por Plataforma

El Makefile incluye optimizaciones espec√≠ficas por arquitectura:

```makefile
# Jetson Nano (Maxwell): sm_53
# NVCC_FLAGS += -arch=sm_53

# Jetson TX2 (Pascal): sm_62  
# NVCC_FLAGS += -arch=sm_62

# Jetson Xavier (Volta): sm_72 [ACTUAL]
NVCC_FLAGS += -arch=sm_72

# Jetson Orin (Ampere): sm_87
# NVCC_FLAGS += -arch=sm_87
```

### Demo Interactivo del Simulador

Para explorar manualmente el entorno antes del entrenamiento:

```bash
cd pc_simulator
python3 visualizer.py
```

#### **Controles de Teclado:**
| Tecla | Acci√≥n | Efecto en Robot |
|-------|--------|-----------------|
| **W** | Avanzar | v += 0.5 m/s |
| **S** | Frenar | v *= 0.5, œâ *= 0.5 |
| **A** | Girar izquierda | œâ += 1.0 rad/s |
| **D** | Girar derecha | œâ -= 1.0 rad/s |
| **X** | Retroceder | v -= 0.3 m/s |
| **R** | Reiniciar | Nueva posici√≥n aleatoria |
| **Q** | Salir | Cerrar simulador |

**Objetivo del demo**: Entender la din√°mica del robot y la dificultad del entorno antes de entrenar la IA.

### Optimizaciones de Rendimiento Implementadas

#### **En el Simulador (Python):**
- **Vectorizaci√≥n NumPy**: C√°lculos de sensores y f√≠sica optimizados
- **Renderizado condicional**: Solo renderiza cuando es necesario
- **TCP sin bloqueo**: Timeouts para evitar cuelgues
- **Cache de colisiones**: Evita rec√°lculos innecesarios

#### **En el Agente (CUDA):**
- **Memory coalescing**: Accesos alineados a memoria GPU
- **Shared memory**: Cache local para pesos de red neuronal  
- **Kernels fusionados**: Forward+backward pass en un solo kernel
- **Streams CUDA**: Paralelizaci√≥n de transfers CPU‚ÜîGPU
- **Target network soft update**: Actualizaci√≥n eficiente en GPU

### Extensiones Futuras Posibles

#### **Mejoras del Entorno:**
- Entornos din√°micos con obst√°culos m√≥viles
- M√∫ltiples robots colaborativos
- Objetivos m√∫ltiples o secuenciales
- Ruido en sensores para realismo

#### **Mejoras del Algoritmo:**
- Dueling DQN para mejor estimaci√≥n de valores
- Prioritized Experience Replay para muestras importantes
- Rainbow DQN con todas las mejoras combinadas
- Multi-agent Deep Q-Network (MADQN)

#### **Integraci√≥n con Robot Real:**
- ROS 2 para interfaz con hardware real
- C√°mara RGB-D para sensores visuales
- LIDAR para navegaci√≥n precisa
- Actuadores servo para control de motores

---

## Conclusiones

Este sistema demuestra la **implementaci√≥n exitosa de un DQN distribuido** con separaci√≥n clara de responsabilidades:

- **PC**: Se enfoca en simulaci√≥n realista y visualizaci√≥n
- **Jetson**: Maximiza el rendimiento de IA con CUDA
- **TCP**: Permite escalabilidad y flexibilidad de despliegue

La arquitectura es **extensible y modular**, facilitando mejoras futuras tanto en algoritmos de IA como en complejidad del entorno de simulaci√≥n.

**Desarrollado en Diciembre 2025** - Proyecto de Control Inteligente con Deep Reinforcement Learning
=======
# DQN_JetsonXavier
>>>>>>> 374e4ed686d0dbdf3cf0f937fa4f373385274a53
