# Sistema de Control Inteligente DQN Distribuido
## Informe TÃ©cnico - PC (Simulador) + Jetson Xavier (Agente de IA)

### Resumen Ejecutivo

Este proyecto implementa un **sistema de control inteligente distribuido** basado en **Deep Q-Network (DQN)** para el control autÃ³nomo de un robot mÃ³vil diferencial. El sistema utiliza una arquitectura cliente-servidor donde:

- **PC (Cliente)**: Ejecuta la simulaciÃ³n del entorno y el robot en Python con visualizaciÃ³n en tiempo real
- **Jetson Xavier (Servidor)**: Ejecuta el agente de inteligencia artificial DQN en C++/CUDA para mÃ¡ximo rendimiento

La comunicaciÃ³n entre ambos sistemas se realiza mediante **protocolo TCP/IP**, permitiendo entrenamiento distribuido con separaciÃ³n de responsabilidades: simulaciÃ³n vs. procesamiento de IA.

### Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         TCP/IP          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                PC (Cliente)             â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚         Jetson Xavier (Servidor)         â”‚
â”‚                                         â”‚                         â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   Estados (10 valores)  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚        Simulador 2D             â”‚â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚  â”‚         Servidor TCP            â”‚   â”‚
â”‚  â”‚  â€¢ FÃ­sica del robot             â”‚   â”‚        Formato CSV       â”‚  â”‚  â€¢ Parseo de estados            â”‚   â”‚
â”‚  â”‚  â€¢ DetecciÃ³n de colisiones      â”‚   â”‚                         â”‚  â”‚  â€¢ ValidaciÃ³n de datos          â”‚   â”‚
â”‚  â”‚  â€¢ Sensores de distancia        â”‚   â”‚   Acciones (0-4)        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”‚  â€¢ CÃ¡lculo de recompensas       â”‚â—„â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚              â”‚                          â”‚
â”‚  â”‚  â€¢ GestiÃ³n de episodios         â”‚   â”‚      (Enteros)          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                         â”‚  â”‚         DQN Agent               â”‚   â”‚
â”‚                                         â”‚                         â”‚  â”‚  â€¢ Red Neuronal (CUDA)         â”‚   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                         â”‚  â”‚  â€¢ Replay Buffer                â”‚   â”‚
â”‚  â”‚      VisualizaciÃ³n GUI          â”‚   â”‚                         â”‚  â”‚  â€¢ Entrenamiento online         â”‚   â”‚
â”‚  â”‚  â€¢ Pygame 2D                    â”‚   â”‚                         â”‚  â”‚  â€¢ Target Network               â”‚   â”‚
â”‚  â”‚  â€¢ Robot, obstÃ¡culos, goal      â”‚   â”‚                         â”‚  â”‚  â€¢ PolÃ­tica Îµ-greedy            â”‚   â”‚
â”‚  â”‚  â€¢ Trayectorias                 â”‚   â”‚                         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                         â”‚                                          â”‚
â”‚                                         â”‚                         â”‚           GPU CUDA Cores                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Estructura del Proyecto y DescripciÃ³n de Archivos

### OrganizaciÃ³n del CÃ³digo

```
Final_DQN/
â”œâ”€â”€ pc_simulator/           # ğŸ–¥ï¸  CÃ³digo PC (Python) - SimulaciÃ³n y VisualizaciÃ³n
â”‚   â”œâ”€â”€ robot_simulator.py  # Simulador principal del robot diferencial
â”‚   â”œâ”€â”€ visualizer.py       # Interfaz grÃ¡fica con Pygame
â”‚   â”œâ”€â”€ tcp_client.py       # Cliente TCP de comunicaciÃ³n
â”‚   â”œâ”€â”€ test_server.py      # Servidor de pruebas para desarrollo local
â”‚   â””â”€â”€ run_tests.py        # Suite de pruebas automatizadas
â”‚
â”œâ”€â”€ jetson_agent/           # ğŸš€ CÃ³digo Jetson (C++/CUDA) - Inteligencia Artificial  
â”‚   â”œâ”€â”€ include/            # Headers de las clases principales
â”‚   â”‚   â”œâ”€â”€ cuda_utils.cuh     # Utilidades y macros CUDA
â”‚   â”‚   â”œâ”€â”€ neural_network.cuh # ImplementaciÃ³n de red neuronal DQN
â”‚   â”‚   â”œâ”€â”€ replay_buffer.hpp  # Buffer de experiencias para entrenamiento
â”‚   â”‚   â”œâ”€â”€ dqn_agent.cuh      # Agente DQN completo con algoritmo
â”‚   â”‚   â””â”€â”€ tcp_server.hpp     # Servidor TCP robusto con manejo de errores
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ main.cu           # Programa principal y loop de entrenamiento
â”‚   â””â”€â”€ Makefile              # CompilaciÃ³n optimizada para CUDA
â”‚
â””â”€â”€ README.md                # Este documento
```

### DescripciÃ³n Detallada de Archivos

#### ğŸ“ PC Simulator (Python)

**`robot_simulator.py`** - *NÃºcleo de la SimulaciÃ³n*
- **PropÃ³sito**: Simula un robot diferencial en entorno 2D con fÃ­sica realista
- **Funcionalidades**:
  - Modelo dinÃ¡mico del robot (velocidad lineal/angular)
  - Sistema de sensores de distancia (3 sensores: frontal, izquierdo, derecho)
  - DetecciÃ³n de colisiones con obstÃ¡culos y lÃ­mites del entorno
  - CÃ¡lculo de recompensas basado en progreso hacia objetivo
  - GestiÃ³n de episodios (reset, terminaciÃ³n)
- **Entorno**: 3Ã—3 metros con 2 obstÃ¡culos estratÃ©gicos
- **Estados**: Vector de 10 dimensiones (posiciÃ³n, orientaciÃ³n, velocidades, sensores, objetivo)

**`visualizer.py`** - *Interfaz GrÃ¡fica*
- **PropÃ³sito**: VisualizaciÃ³n en tiempo real del entrenamiento con Pygame
- **Elementos visuales**:
  - Robot (cÃ­rculo azul con orientaciÃ³n)
  - ObstÃ¡culos (cÃ­rculos rojos)
  - Objetivo (cÃ­rculo verde)
  - Trayectoria del robot
  - InformaciÃ³n de estado (episodio, recompensa, pasos)
- **Modos**: VisualizaciÃ³n continua o por intervalos para optimizaciÃ³n

**`tcp_client.py`** - *ComunicaciÃ³n TCP*
- **PropÃ³sito**: Cliente TCP que conecta con el agente DQN en Jetson
- **Protocolo**: EnvÃ­o sÃ­ncrono de estados y recepciÃ³n de acciones
- **Manejo de errores**: ReconexiÃ³n automÃ¡tica y sincronizaciÃ³n robusta
- **Formato de datos**: CSV para estados, enteros para acciones

#### ğŸ”§ Jetson Agent (C++/CUDA)

**`neural_network.cuh`** - *Red Neuronal DQN*
- **PropÃ³sito**: ImplementaciÃ³n de red neuronal profunda en CUDA
- **Arquitectura**: 
  ```
  Input(10) â†’ Dense(128,ReLU) â†’ Dense(128,ReLU) â†’ Dense(64,ReLU) â†’ Output(5)
  ```
- **OptimizaciÃ³n**: Kernels CUDA personalizados para forward/backward pass
- **Memoria**: GestiÃ³n eficiente de memoria GPU
- **Target Network**: ImplementaciÃ³n de red objetivo para estabilidad

**`replay_buffer.hpp`** - *Buffer de Experiencias*
- **PropÃ³sito**: Almacenar experiencias (s,a,r,s') para entrenamiento por lotes
- **Capacidad**: 100,000 experiencias con sobrescritura circular
- **Muestreo**: SelecciÃ³n aleatoria de mini-lotes de 64 experiencias
- **OptimizaciÃ³n**: Acceso eficiente a memoria para entrenamiento continuo

**`dqn_agent.cuh`** - *Agente DQN Principal*
- **PropÃ³sito**: Implementa el algoritmo completo de Deep Q-Network
- **CaracterÃ­sticas**:
  - PolÃ­tica Îµ-greedy con decaimiento (1.0 â†’ 0.05)
  - Entrenamiento online cada paso
  - Soft update de target network (Ï„ = 0.005)
  - Guardado/carga de modelos entrenados
- **HiperparÃ¡metros optimizados**: lr=0.0005, Î³=0.99, batch=64

**`tcp_server.hpp`** - *Servidor TCP Robusto*
- **PropÃ³sito**: Servidor TCP que recibe estados y envÃ­a acciones
- **Robustez**: Parseo seguro con manejo de errores de formato
- **Funciones de seguridad**: `safe_stof()`, `safe_stoi()` para evitar excepciones
- **GestiÃ³n de conexiones**: Manejo de desconexiones y reconexiones

**`main.cu`** - *Programa Principal*
- **PropÃ³sito**: OrquestaciÃ³n del entrenamiento completo
- **Loop principal**:
  1. InicializaciÃ³n de CUDA y red neuronal
  2. CreaciÃ³n de servidor TCP en puerto 5555
  3. Ciclo de entrenamiento episÃ³dico
  4. Guardado periÃ³dico de modelos
- **MÃ©tricas**: Logging de recompensas, epsilon, loss durante entrenamiento

## Protocolo de ComunicaciÃ³n TCP/IP

### Arquitectura de Red

El sistema utiliza un **protocolo TCP personalizado** para comunicaciÃ³n distribuida:

- **Jetson Xavier**: ActÃºa como **SERVIDOR** (puerto 5555)
- **PC**: ActÃºa como **CLIENTE** (conecta al Jetson)
- **Protocolo**: SÃ­ncrono, un mensaje por paso de simulaciÃ³n

### Datos Enviados: PC â†’ Jetson (Estado del Robot)

El PC envÃ­a el estado completo del robot en **formato CSV** con 10 valores:

```csv
x,y,theta,v,omega,d_front,d_left,d_right,dx_goal,dy_goal|done,reward,goal,collision
```

| Campo | Tipo | Rango | DescripciÃ³n |
|-------|------|-------|-------------|
| `x` | float | [0.0, 3.0] | PosiciÃ³n X del robot (metros) |
| `y` | float | [0.0, 3.0] | PosiciÃ³n Y del robot (metros) |
| `theta` | float | [-Ï€, Ï€] | OrientaciÃ³n del robot (radianes) |
| `v` | float | [0.0, 2.0] | Velocidad lineal actual (m/s) |
| `omega` | float | [-2.0, 2.0] | Velocidad angular actual (rad/s) |
| `d_front` | float | [0.0, 5.0] | Distancia sensor frontal (metros) |
| `d_left` | float | [0.0, 5.0] | Distancia sensor izquierdo (metros) |
| `d_right` | float | [0.0, 5.0] | Distancia sensor derecho (metros) |
| `dx_goal` | float | [-3.0, 3.0] | Componente X hacia objetivo |
| `dy_goal` | float | [-3.0, 3.0] | Componente Y hacia objetivo |

**Ejemplo de mensaje real:**
```
1.5,0.8,0.785,1.2,-0.3,2.1,1.8,3.2,-1.0,1.7|0,-0.1,0,0
```

### Datos Recibidos: Jetson â†’ PC (AcciÃ³n Seleccionada)

El Jetson responde con un **entero** que representa la acciÃ³n a ejecutar:

| AcciÃ³n | Valor | Efecto en el Robot |
|--------|-------|-------------------|
| **FORWARD** | `0` | Acelerar hacia adelante (v += 0.5) |
| **LEFT** | `1` | Girar a la izquierda (Ï‰ += 1.0) |
| **RIGHT** | `2` | Girar a la derecha (Ï‰ -= 1.0) |
| **BRAKE** | `3` | Frenar (v *= 0.5, Ï‰ *= 0.5) |
| **BACKWARD** | `4` | Retroceder (v -= 0.3) |

### Procesamiento en Jetson Xavier

#### 1. **RecepciÃ³n y Parseo de Estados**
```cpp
// tcp_server.hpp - Parseo robusto
std::vector<float> parseState(const std::string& message) {
    // Separar por comas y convertir con safe_stof()
    // Validar rangos y detectar errores de formato
    // Retornar vector normalizado para la red neuronal
}
```

#### 2. **Inferencia DQN (Forward Pass)**
```cpp
// neural_network.cuh - Procesamiento CUDA
__global__ void forward_pass_kernel(float* input, float* output, 
                                   float* weights, int batch_size) {
    // 1. PropagaciÃ³n hacia adelante en GPU
    // 2. Activaciones ReLU entre capas
    // 3. CÃ¡lculo de Q-values para las 5 acciones
}
```

**Flujo de procesamiento:**
1. **NormalizaciÃ³n**: Estados se normalizan a rango [0,1]
2. **GPU Transfer**: Datos se copian a memoria GPU
3. **Forward Pass**: Red neuronal procesa entrada â†’ Q-values
4. **SelecciÃ³n de AcciÃ³n**: Îµ-greedy sobre Q-values mÃ¡ximos
5. **CPU Return**: AcciÃ³n seleccionada regresa a CPU

#### 3. **Entrenamiento DQN (Backward Pass)**
```cpp
// dqn_agent.cuh - Entrenamiento online
void train_step() {
    // 1. Muestrear mini-lote del replay buffer (64 experiencias)
    // 2. Calcular Q-targets usando target network
    // 3. Forward pass en main network
    // 4. Calcular loss MSE: (Q_pred - Q_target)Â²
    // 5. Backward pass y actualizaciÃ³n de pesos (Adam optimizer)
    // 6. Soft update de target network
}
```

### GestiÃ³n de Errores y Robustez

#### En el PC (Cliente):
- **ReconexiÃ³n automÃ¡tica** si se pierde conexiÃ³n
- **Timeout** de 5 segundos por mensaje
- **ValidaciÃ³n** de respuestas del Jetson
- **SincronizaciÃ³n** robusta entre episodios

#### En el Jetson (Servidor):
- **Parseo seguro** con funciones `safe_stof()`
- **ValidaciÃ³n de rangos** de los estados recibidos
- **Manejo de clientes mÃºltiples** (aunque solo uno activo)
- **Recovery** automÃ¡tico de errores de formato

## Algoritmo Deep Q-Network (DQN)

### Fundamentos TeÃ³ricos

El **Deep Q-Network** es un algoritmo de aprendizaje por refuerzo que combina:
- **Q-Learning**: Algoritmo de diferencias temporales para estimar valores Q(s,a)
- **Redes Neuronales Profundas**: AproximaciÃ³n de funciones para espacios de estados continuos
- **Experience Replay**: Buffer de experiencias para entrenamiento estable
- **Target Network**: Red objetivo para estabilizar el entrenamiento

### ImplementaciÃ³n en CUDA

#### **Arquitectura de Red Neuronal**
```
Entrada (10 dimensiones)
    â†“
Capa Densa 1: 10 â†’ 128 neuronas + ReLU
    â†“  
Capa Densa 2: 128 â†’ 128 neuronas + ReLU
    â†“
Capa Densa 3: 128 â†’ 64 neuronas + ReLU  
    â†“
Salida: 64 â†’ 5 Q-values (una por acciÃ³n)
```

**ParÃ¡metros totales**: ~35,000 pesos entrenables

#### **FunciÃ³n de PÃ©rdida (Loss Function)**
```
L(Î¸) = E[(Q_target - Q_pred)Â²]

Donde:
Q_target = r + Î³ * max_a' Q_target(s', a')
Q_pred = Q_main(s, a)
```

#### **HiperparÃ¡metros Optimizados**

| ParÃ¡metro | Valor | JustificaciÃ³n |
|-----------|-------|---------------|
| **Learning Rate** | 0.0005 | Convergencia estable sin overshooting |
| **Gamma (Î³)** | 0.99 | Prioriza recompensas futuras (visiÃ³n a largo plazo) |
| **Epsilon inicial** | 1.0 | ExploraciÃ³n mÃ¡xima al inicio |
| **Epsilon final** | 0.05 | Mantiene 5% exploraciÃ³n para adaptabilidad |
| **Epsilon decay** | 0.9999 | Decaimiento gradual (5000 episodios) |
| **Batch Size** | 64 | Balance entre estabilidad y eficiencia GPU |
| **Replay Buffer** | 100,000 | Suficiente diversidad sin consumir memoria |
| **Tau (Ï„)** | 0.005 | Soft update lento para estabilidad |
| **Train Frequency** | 1 | Entrenamiento en cada paso (online) |

### FunciÃ³n de Recompensa DiseÃ±ada

La funciÃ³n de recompensa estÃ¡ **cuidadosamente diseÃ±ada** para guiar el aprendizaje:

```python
def calculate_reward(self):
    reward = 0.0
    
    # ğŸ¯ OBJETIVO PRINCIPAL
    if self.check_goal_reached():
        return +100.0  # Recompensa mÃ¡xima por Ã©xito
    
    # âš ï¸ PENALIZACIÃ“N POR COLISIÃ“N  
    if self.check_collision():
        return -100.0  # PenalizaciÃ³n mÃ¡xima por fallo
    
    # ğŸ“ˆ PROGRESO HACIA EL OBJETIVO
    dist_atual = np.linalg.norm(self.position - self.goal)
    if dist_atual < self.prev_distance:
        reward += 10.0 * (self.prev_distance - dist_atual)  # Recompensa por acercarse
    
    # â±ï¸ COSTO POR TIEMPO
    reward -= 0.1  # Incentiva soluciones rÃ¡pidas
    
    # ğŸš« PENALIZACIÃ“N POR ACCIONES INEFICIENTES
    if action == BRAKE:
        reward -= 0.2  # Desincentivar frenado excesivo
    elif action == BACKWARD:
        reward -= 0.15  # Desincentivar retroceso
    
    # ğŸ¯ BONIFICACIÃ“N POR PROXIMIDAD AL OBJETIVO
    if dist_atual < 1.0:
        reward += 0.5  # Cerca del objetivo
    elif dist_atual < 2.0:
        reward += 0.2  # Moderadamente cerca
        
    # âš ï¸ PENALIZACIÃ“N POR PROXIMIDAD A OBSTÃCULOS
    if min(d_front, d_left, d_right) < 0.3:
        reward -= 0.5  # Incentiva mantener distancia segura
        
    return reward
```

### Proceso de Entrenamiento

#### **Ciclo de Entrenamiento por Episodio:**

1. **InicializaciÃ³n**
   - Robot se posiciona en (0.1, 0.1)
   - Objetivo en (2.5, 2.5) 
   - ObstÃ¡culos fijos en posiciones estratÃ©gicas

2. **Loop de Pasos** (mÃ¡ximo 150 pasos por episodio)
   ```cpp
   for (int step = 0; step < max_steps; step++) {
       // 1. Recibir estado del PC
       state = tcp_server.receive_state();
       
       // 2. Seleccionar acciÃ³n (Îµ-greedy)
       action = agent.select_action(state, epsilon);
       
       // 3. Enviar acciÃ³n al PC
       tcp_server.send_action(action);
       
       // 4. Almacenar experiencia
       replay_buffer.add(prev_state, action, reward, state, done);
       
       // 5. Entrenar si hay suficientes experiencias
       if (replay_buffer.size() >= min_replay_size) {
           agent.train_step();
       }
   }
   ```

3. **ActualizaciÃ³n de ParÃ¡metros**
   - Decaimiento de epsilon: `Îµ = Îµ * decay_rate`
   - Soft update de target network: `Î¸_target = Ï„*Î¸_main + (1-Ï„)*Î¸_target`
   - Guardado de modelo cada 100 episodios

#### **MÃ©tricas de Entrenamiento Monitoreadas:**

- **Recompensa acumulada por episodio**
- **Tasa de Ã©xito** (episodios que alcanzan objetivo)
- **NÃºmero de pasos promedio** hasta completar tarea
- **Loss de la red neuronal** (MSE)
- **Valor de epsilon actual** (exploraciÃ³n vs explotaciÃ³n)

## ConfiguraciÃ³n de Red y EjecuciÃ³n

### ConfiguraciÃ³n de Red Distribuida

#### **Paso 1: Configurar IPs de los Dispositivos**

**En el Jetson Xavier (Servidor):**
```bash
# Obtener IP del Jetson
ip addr show eth0        # Ethernet (recomendado para estabilidad)
# O para WiFi:
ip addr show wlan0

# Ejemplo de salida: inet 192.168.18.114/24
```

**En el PC (Cliente):**
```bash
# Verificar conectividad con el Jetson
ping 192.168.18.114     # Usar la IP real del Jetson

# Opcional: Verificar puerto abierto
nc -zv 192.168.18.114 5555
```

#### **Paso 2: Configurar Firewall (si es necesario)**

**En el Jetson Xavier:**
```bash
# Permitir puerto 5555 para comunicaciÃ³n TCP
sudo ufw allow 5555/tcp

# O desactivar firewall temporalmente durante desarrollo
sudo ufw disable
```

### InstalaciÃ³n y CompilaciÃ³n

#### **En el PC (Python 3.13+)**
```bash
# Instalar dependencias del simulador
pip3 install numpy pygame

# Verificar instalaciÃ³n
cd pc_simulator
python3 -c "import numpy, pygame; print('Dependencias OK')"

# Ejecutar pruebas del simulador
python3 run_tests.py --test simulator
```

#### **En el Jetson Xavier (CUDA 12.2+)**
```bash
# Transferir cÃ³digo al Jetson
scp -r jetson_agent/ usuario@192.168.18.114:~/Final_DQN/

# Conectar al Jetson y compilar
ssh usuario@192.168.18.114
cd ~/Final_DQN/jetson_agent

# Verificar CUDA disponible
nvidia-smi
nvcc --version

# Compilar el agente DQN
make clean && make

# Verificar compilaciÃ³n exitosa
ls -la bin/dqn_agent    # Debe existir el ejecutable
```

### EjecuciÃ³n del Sistema Distribuido

#### **Secuencia de Inicio (IMPORTANTE: Orden especÃ­fico)**

**1. Iniciar Servidor DQN en Jetson (PRIMERO):**
```bash
# En terminal del Jetson Xavier
cd ~/Final_DQN/jetson_agent
./bin/dqn_agent --port 5555 --episodes 500

# Salida esperada:
# [CUDA] Inicializando dispositivo GPU...
# [DQN] Creando red neuronal 10->128->128->64->5
# [TCP] Servidor esperando conexiones en puerto 5555...
```

**2. Iniciar Cliente Simulador en PC (SEGUNDO):**
```bash
# En terminal del PC
cd /home/saul/Documentos/Final_DQN/pc_simulator
python3 tcp_client.py --ip 192.168.18.114 --port 5555 --episodes 500 --visualize

# Salida esperada:
# [TCP] Conectando a 192.168.18.114:5555...
# [SIMULATOR] Iniciando entrenamiento DQN...
# [EPISODE 1] Recompensa: -45.2, Pasos: 150, Epsilon: 0.998
```

### ParÃ¡metros de ConfiguraciÃ³n

#### **Opciones del Simulador (PC)**
| ParÃ¡metro | DescripciÃ³n | Valor Default | Rango |
|-----------|-------------|---------------|-------|
| `--ip` | IP del Jetson Xavier | `127.0.0.1` | IP vÃ¡lida |
| `--port` | Puerto TCP de comunicaciÃ³n | `5555` | 1024-65535 |
| `--episodes` | NÃºmero total de episodios | `1000` | 1-âˆ |
| `--visualize` | Mostrar GUI en tiempo real | `False` | True/False |
| `--render-every` | Renderizar cada N episodios | `10` | 1-100 |
| `--save-logs` | Guardar mÃ©tricas en archivo | `True` | True/False |

#### **Opciones del Agente DQN (Jetson)**
| ParÃ¡metro | DescripciÃ³n | Valor Default | Rango |
|-----------|-------------|---------------|-------|
| `--port` | Puerto TCP del servidor | `5555` | 1024-65535 |
| `--episodes` | Episodios mÃ¡ximos (-1=âˆ) | `-1` | -1,1-âˆ |
| `--model-path` | Ruta del modelo DQN | `models/dqn_model.bin` | Path vÃ¡lido |
| `--load-model` | Cargar modelo existente | `False` | True/False |
| `--save-every` | Guardar modelo cada N episodios | `100` | 10-1000 |
| `--device-id` | ID del dispositivo CUDA | `0` | 0-N |

### Monitoreo del Entrenamiento

#### **En el Jetson (Logs de IA):**
```
[EPISODE 0001] Reward: -85.4  | Loss: 2.45  | Epsilon: 0.999 | Steps: 150
[EPISODE 0050] Reward: -12.3  | Loss: 0.87  | Epsilon: 0.951 | Steps: 89
[EPISODE 0100] Reward: +45.7  | Loss: 0.34  | Epsilon: 0.905 | Steps: 67
[EPISODE 0200] Reward: +89.2  | Loss: 0.18  | Epsilon: 0.819 | Steps: 34
```

#### **En el PC (Logs de SimulaciÃ³n):**
```
[SIM] Episodio 1/500 | Goal: NO | ColisiÃ³n: SÃ | Pasos: 150 | R_total: -85.4
[SIM] Episodio 50/500 | Goal: NO | ColisiÃ³n: SÃ | Pasos: 89 | R_total: -12.3  
[SIM] Episodio 100/500 | Goal: SÃ | ColisiÃ³n: NO | Pasos: 67 | R_total: +45.7
[SIM] Episodio 200/500 | Goal: SÃ | ColisiÃ³n: NO | Pasos: 34 | R_total: +89.2
```

## Resultados Esperados y AnÃ¡lisis de Rendimiento

### Curva de Aprendizaje Esperada

El entrenamiento del DQN sigue un patrÃ³n caracterÃ­stico dividido en **4 fases**:

#### **Fase 1: ExploraciÃ³n Inicial (Episodios 1-50)**
- **Recompensa promedio**: -80 a -50
- **Tasa de Ã©xito**: 0-5%
- **Comportamiento**: Movimientos aleatorios, muchas colisiones
- **Epsilon**: 1.0 â†’ 0.95 (95% exploraciÃ³n)

#### **Fase 2: Aprendizaje BÃ¡sico (Episodios 51-150)**  
- **Recompensa promedio**: -50 a -10
- **Tasa de Ã©xito**: 5-25%
- **Comportamiento**: Comienza a evitar obstÃ¡culos, movimientos mÃ¡s dirigidos
- **Epsilon**: 0.95 â†’ 0.86 (86% exploraciÃ³n)

#### **Fase 3: Refinamiento (Episodios 151-350)**
- **Recompensa promedio**: -10 a +60
- **Tasa de Ã©xito**: 25-70%
- **Comportamiento**: Encuentra rutas vÃ¡lidas consistentemente
- **Epsilon**: 0.86 â†’ 0.70 (70% exploraciÃ³n)

#### **Fase 4: Convergencia (Episodios 351-500+)**
- **Recompensa promedio**: +60 a +95
- **Tasa de Ã©xito**: 70-95%
- **Comportamiento**: PolÃ­tica casi Ã³ptima, rutas eficientes
- **Epsilon**: 0.70 â†’ 0.05 (5% exploraciÃ³n residual)

### MÃ©tricas de EvaluaciÃ³n

#### **MÃ©tricas Primarias:**
- **Tasa de Ã‰xito**: % de episodios que alcanzan el objetivo
- **Recompensa Acumulada**: Suma de recompensas por episodio
- **Pasos hasta Objetivo**: Eficiencia de las rutas encontradas
- **Tiempo de Convergencia**: Episodios necesarios para polÃ­tica estable

#### **MÃ©tricas TÃ©cnicas:**
- **Loss de Red Neuronal**: Error MSE entre Q_pred y Q_target
- **UtilizaciÃ³n de GPU**: % de uso de CUDA cores durante entrenamiento  
- **Throughput**: Pasos procesados por segundo
- **Memoria GPU**: Uso de VRAM para redes y replay buffer

## Pruebas y ValidaciÃ³n

### ConfiguraciÃ³n del Entorno de Prueba

#### **Entorno Optimizado para Aprendizaje RÃ¡pido:**
- **Dimensiones**: 3Ã—3 metros (reducido para acelerar convergencia)
- **PosiciÃ³n inicial robot**: (0.1, 0.1) 
- **Objetivo**: (2.5, 2.5)
- **ObstÃ¡culos**: 2 obstÃ¡culos estratÃ©gicamente ubicados
  - ObstÃ¡culo 1: Centro (1.5, 1.5), radio 0.35m - bloquea ruta directa
  - ObstÃ¡culo 2: (2.2, 1.0), radio 0.2m - bloquea diagonal inferior
- **Pasos mÃ¡ximos**: 150 por episodio

#### **ValidaciÃ³n de Dificultad del Entorno:**
Antes del entrenamiento DQN, se validÃ³ que el entorno requiere aprendizaje:

```bash
# Prueba con polÃ­tica directa (sin evasiÃ³n)
=== Test: PolÃ­tica DIRECTA (sin evasiÃ³n) ===
Resultado: 0/10 goals, 10/10 colisiones

# Prueba con polÃ­tica aleatoria  
=== Test: PolÃ­tica ALEATORIA ===
Resultado: 0/10 goals, 10/10 colisiones

>> Un DQN entrenado deberÃ­a superar ambas polÃ­ticas!
```

**ConclusiÃ³n**: Ambiente desafiante que requiere aprendizaje para tener Ã©xito.

### Pruebas Locales de Desarrollo

Para desarrollo y debugging sin el Jetson, usa el servidor de prueba:

#### **Servidor de Prueba en Python:**
```bash
# Terminal 1: Servidor de prueba (simula Jetson)
cd pc_simulator  
python3 test_server.py --port 5555 --episodes 100 --policy random

# Terminal 2: Cliente simulador
cd pc_simulator
python3 tcp_client.py --ip 127.0.0.1 --port 5555 --episodes 100 --visualize
```

#### **PolÃ­ticas de Prueba Disponibles:**
| PolÃ­tica | DescripciÃ³n | Uso |
|----------|-------------|-----|
| `random` | Acciones aleatorias uniformes | Baseline inferior |
| `forward` | Solo avanzar (sin evasiÃ³n) | Validar obstÃ¡culos |
| `simple` | Giros simples al detectar obstÃ¡culos | HeurÃ­stica bÃ¡sica |
| `dqn` | Cargar modelo DQN entrenado | Validar agente |

### Suite de Pruebas Automatizadas

#### **Ejecutar todas las pruebas:**
```bash
cd pc_simulator
python3 run_tests.py --all

# O pruebas especÃ­ficas:
python3 run_tests.py --test simulator      # Prueba simulador solo  
python3 run_tests.py --test tcp           # Prueba comunicaciÃ³n TCP
python3 run_tests.py --test environment   # Prueba configuraciÃ³n entorno
```

#### **Pruebas de Rendimiento:**
```bash
# Benchmark de throughput del simulador
python3 run_tests.py --test performance --episodes 1000

# Salida esperada:
# [PERF] SimulaciÃ³n: 2847 pasos/segundo
# [PERF] TCP: 1923 mensajes/segundo  
# [PERF] RenderizaciÃ³n: 60 FPS promedio
```

## SoluciÃ³n de Problemas y Debugging

### Problemas Comunes de Conectividad

#### **Error: "Connection refused" o "No route to host"**
```bash
# 1. Verificar que el Jetson estÃ© ejecutando el servidor
ssh usuario@192.168.18.114
ps aux | grep dqn_agent    # Debe aparecer el proceso

# 2. Verificar IP correcta del Jetson
ip addr show eth0          # Confirmar IP real

# 3. Probar conectividad bÃ¡sica
ping 192.168.18.114        # Desde el PC
nc -zv 192.168.18.114 5555 # Probar puerto especÃ­fico

# 4. Configurar firewall en Jetson
sudo ufw allow 5555/tcp
# O temporalmente: sudo ufw disable
```

#### **Error: "CUDA out of memory"**
```bash
# 1. Verificar memoria GPU disponible
nvidia-smi

# 2. Si hay otros procesos usando GPU, terminarlos
sudo fuser -v /dev/nvidia*
sudo kill -9 <PID_DEL_PROCESO>

# 3. Reducir batch size en main.cu si es necesario
# Cambiar BATCH_SIZE de 64 a 32 o 16
```

#### **Error: "stof exception" o parseo de datos**
```bash
# Verificar formato de mensajes TCP
# En pc_simulator/tcp_client.py, aÃ±adir debug:
print(f"Enviando: {message}")

# En jetson_agent/include/tcp_server.hpp
# Ya tiene manejo robusto con safe_stof()
```

### Problemas de Entrenamiento

#### **El agente no aprende (recompensa no mejora)**
1. **Verificar replay buffer**: Debe acumular al menos 500 experiencias
2. **Ajustar epsilon decay**: Muy rÃ¡pido impide exploraciÃ³n
3. **Revisar funciÃ³n de recompensa**: Debe dar feedback Ãºtil
4. **Aumentar episodios**: DQN necesita 200-500 episodios mÃ­nimo

#### **Convergencia muy lenta**  
1. **Reducir tamaÃ±o del entorno**: Ya optimizado a 3Ã—3m
2. **Ajustar learning rate**: Probar 0.001 si 0.0005 es muy lento
3. **Simplificar obstÃ¡culos**: Reducir de 2 a 1 obstÃ¡culo temporalmente

#### **Inestabilidad en el entrenamiento**
1. **Verificar target network**: Debe actualizarse cada 100 pasos
2. **Revisar soft update tau**: 0.005 es conservativo y estable
3. **Monitorear loss**: No debe crecer indefinidamente

### Debugging Avanzado

#### **Logs Detallados en Jetson:**
```cpp
// En main.cu, aÃ±adir:
#define DEBUG_MODE 1

// Habilita logs extendidos:
// [DEBUG] Estado recibido: [1.2, 0.8, 0.785, ...]
// [DEBUG] Q-values: [0.23, -0.45, 0.78, -0.12, 0.34]
// [DEBUG] AcciÃ³n seleccionada: 2 (epsilon=0.891)
```

#### **Profiling de Rendimiento:**
```bash
# En Jetson, usar nvprof para anÃ¡lisis CUDA
nvprof --log-file dqn_profile.txt ./bin/dqn_agent --episodes 10

# Analizar cuellos de botella:
# GPU utilization, memory transfers, kernel execution time
```

#### **VisualizaciÃ³n de MÃ©tricas:**
```python
# En PC, modificar tcp_client.py para guardar mÃ©tricas
import matplotlib.pyplot as plt

rewards = []  # Recolectar durante entrenamiento
plt.plot(rewards)
plt.xlabel('Episodio')
plt.ylabel('Recompensa Acumulada')  
plt.title('Curva de Aprendizaje DQN')
plt.show()
```

## Notas TÃ©cnicas sobre Hardware

### Requisitos del Sistema

#### **PC (Cliente - Simulador):**
- **CPU**: Intel/AMD multi-core (4+ cores recomendado)
- **RAM**: 4 GB mÃ­nimo, 8 GB recomendado  
- **Python**: 3.8+ (probado con Python 3.13)
- **Dependencias**: NumPy, Pygame
- **Red**: ConexiÃ³n Ethernet/WiFi estable con Jetson

#### **Jetson Xavier (Servidor - Agente IA):**
- **GPU**: 512 CUDA cores (Volta), 32 Tensor cores
- **RAM**: 16/32 GB (probado con 32 GB)
- **CUDA**: Compute Capability 7.2 (sm_72)
- **Storage**: 10 GB disponible para modelos y logs
- **Red**: Ethernet preferido para estabilidad

### ConfiguraciÃ³n CUDA por Plataforma

El Makefile incluye optimizaciones especÃ­ficas por arquitectura:

```makefile
# Jetson Nano (Maxwell): sm_53
# NVCC_FLAGS += -arch=sm_53

# Jetson TX2 (Pascal): sm_62  
# NVCC_FLAGS += -arch=sm_62

# Jetson Xavier (Volta): sm_72 [ACTUAL]
NVCC_FLAGS += -arch=sm_72

# Jetson Orin (Ampere): sm_87
# NVCC_FLAGS += -arch=sm_87
```

### Demo Interactivo del Simulador

Para explorar manualmente el entorno antes del entrenamiento:

```bash
cd pc_simulator
python3 visualizer.py
```

#### **Controles de Teclado:**
| Tecla | AcciÃ³n | Efecto en Robot |
|-------|--------|-----------------|
| **W** | Avanzar | v += 0.5 m/s |
| **S** | Frenar | v *= 0.5, Ï‰ *= 0.5 |
| **A** | Girar izquierda | Ï‰ += 1.0 rad/s |
| **D** | Girar derecha | Ï‰ -= 1.0 rad/s |
| **X** | Retroceder | v -= 0.3 m/s |
| **R** | Reiniciar | Nueva posiciÃ³n aleatoria |
| **Q** | Salir | Cerrar simulador |

**Objetivo del demo**: Entender la dinÃ¡mica del robot y la dificultad del entorno antes de entrenar la IA.

### Optimizaciones de Rendimiento Implementadas

#### **En el Simulador (Python):**
- **VectorizaciÃ³n NumPy**: CÃ¡lculos de sensores y fÃ­sica optimizados
- **Renderizado condicional**: Solo renderiza cuando es necesario
- **TCP sin bloqueo**: Timeouts para evitar cuelgues
- **Cache de colisiones**: Evita recÃ¡lculos innecesarios

#### **En el Agente (CUDA):**
- **Memory coalescing**: Accesos alineados a memoria GPU
- **Shared memory**: Cache local para pesos de red neuronal  
- **Kernels fusionados**: Forward+backward pass en un solo kernel
- **Streams CUDA**: ParalelizaciÃ³n de transfers CPUâ†”GPU
- **Target network soft update**: ActualizaciÃ³n eficiente en GPU

### Extensiones Futuras Posibles

#### **Mejoras del Entorno:**
- Entornos dinÃ¡micos con obstÃ¡culos mÃ³viles
- MÃºltiples robots colaborativos
- Objetivos mÃºltiples o secuenciales
- Ruido en sensores para realismo

#### **Mejoras del Algoritmo:**
- Dueling DQN para mejor estimaciÃ³n de valores
- Prioritized Experience Replay para muestras importantes
- Rainbow DQN con todas las mejoras combinadas
- Multi-agent Deep Q-Network (MADQN)

#### **IntegraciÃ³n con Robot Real:**
- ROS 2 para interfaz con hardware real
- CÃ¡mara RGB-D para sensores visuales
- LIDAR para navegaciÃ³n precisa
- Actuadores servo para control de motores

---

## Conclusiones

Este sistema demuestra la **implementaciÃ³n exitosa de un DQN distribuido** con separaciÃ³n clara de responsabilidades:

- **PC**: Se enfoca en simulaciÃ³n realista y visualizaciÃ³n
- **Jetson**: Maximiza el rendimiento de IA con CUDA
- **TCP**: Permite escalabilidad y flexibilidad de despliegue

La arquitectura es **extensible y modular**, facilitando mejoras futuras tanto en algoritmos de IA como en complejidad del entorno de simulaciÃ³n.

**Desarrollado en Diciembre 2025** - Proyecto de Control Inteligente con Deep Reinforcement Learning
