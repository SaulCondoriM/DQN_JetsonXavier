# Sistema de Control Inteligente DQN Distribuido PC (Simulador) + Jetson Xavier (Agente de IA)

### Integrantes del Proyecto

| Nombre | Aporte |
|--------|--------|
| Saul Condori Machaca | 22.0 |
| Christian Pardave Espinoza | 19.5 |
| Merisable Ruelas Quenaya | 19.5 |
| Yanira Suni Quispe | 19.5 |
| Katherine Bejar Roman | 19.5 |

### Resumen

Este proyecto implementa un sistema de control inteligente distribuido basado en Deep Q-Network (DQN) para el control aut√≥nomo de un robot m√≥vil diferencial. El sistema utiliza una arquitectura cliente-servidor donde:

- **PC (Cliente)**: Ejecuta la simulaci√≥n del entorno y el robot en Python con visualizaci√≥n en tiempo real
- **Jetson Xavier (Servidor)**: Ejecuta el agente de inteligencia artificial DQN en C++/CUDA para m√°ximo rendimiento

La comunicaci√≥n entre ambos sistemas se realiza mediante protocolo TCP/IP, permitiendo entrenamiento distribuido con separaci√≥n de responsabilidades: simulaci√≥n vs. procesamiento de IA.

### Arquitectura del Sistema

```
PC (Cliente)                         TCP/IP                    Jetson Xavier (Servidor)
                                                              
Simulador 2D            ------>   Estados (10 valores)  ------>   Servidor TCP
- F√≠sica del robot                  Formato CSV                  - Parseo de estados
- Detecci√≥n colisiones                                           - Validaci√≥n de datos
- Sensores distancia                                                     |
- C√°lculo recompensas   <------    Acciones (0-4)      <------    DQN Agent
- Gesti√≥n episodios                  (Enteros)                   - Red Neuronal (CUDA)
                                                                 - Replay Buffer
Visualizaci√≥n GUI                                                - Entrenamiento online
- Pygame 2D                                                      - Target Network
- Robot, obst√°culos, goal                                        - Pol√≠tica epsilon-greedy
- Trayectorias                                                   
                                                                 GPU CUDA Cores
```

## Estructura del Proyecto y Descripci√≥n de Archivos

### Organizaci√≥n del C√≥digo

```
Final_DQN/
‚îú‚îÄ‚îÄ pc_simulator/           # üñ•Ô∏è  C√≥digo PC (Python) - Simulaci√≥n y Visualizaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ robot_simulator.py  # Simulador principal del robot diferencial
‚îÇ   ‚îú‚îÄ‚îÄ visualizer.py       # Interfaz gr√°fica con Pygame
‚îÇ   ‚îú‚îÄ‚îÄ tcp_client.py       # Cliente TCP de comunicaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ test_server.py      # Servidor de pruebas para desarrollo local
‚îÇ   ‚îî‚îÄ‚îÄ run_tests.py        # Suite de pruebas automatizadas
‚îÇ
‚îú‚îÄ‚îÄ jetson_agent/           # üöÄ C√≥digo Jetson (C++/CUDA) - Inteligencia Artificial  
‚îÇ   ‚îú‚îÄ‚îÄ include/            # Headers de las clases principales
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cuda_utils.cuh     # Utilidades y macros CUDA
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ neural_network.cuh # Implementaci√≥n de red neuronal DQN
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ replay_buffer.hpp  # Buffer de experiencias para entrenamiento
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dqn_agent.cuh      # Agente DQN completo con algoritmo
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tcp_server.hpp     # Servidor TCP robusto con manejo de errores
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.cu           # Programa principal y loop de entrenamiento
‚îÇ   ‚îî‚îÄ‚îÄ Makefile              # Compilaci√≥n optimizada para CUDA
‚îÇ
‚îî‚îÄ‚îÄ README.md                # Este documento
```

### Descripci√≥n Detallada de Archivos

#### PC Simulator (Python)

**`robot_simulator.py`** - N√∫cleo de la Simulaci√≥n
- Simula un robot diferencial en entorno 2D con f√≠sica realista
- Modelo din√°mico del robot (velocidad lineal/angular)
- Sistema de sensores de distancia (3 sensores: frontal, izquierdo, derecho)
- Detecci√≥n de colisiones con obst√°culos y l√≠mites del entorno
- C√°lculo de recompensas basado en progreso hacia objetivo
- Gesti√≥n de episodios (reset, terminaci√≥n)
- Entorno: 3x3 metros con 2 obst√°culos estrat√©gicos
- Estados: Vector de 10 dimensiones (posici√≥n, orientaci√≥n, velocidades, sensores, objetivo)

**`visualizer.py`** - Interfaz Gr√°fica
- Visualizaci√≥n en tiempo real del entrenamiento con Pygame
- Elementos visuales: Robot (c√≠rculo azul con orientaci√≥n), Obst√°culos (c√≠rculos rojos), Objetivo (c√≠rculo verde), Trayectoria del robot
- Informaci√≥n de estado (episodio, recompensa, pasos)
- Modos: Visualizaci√≥n continua o por intervalos para optimizaci√≥n

**`tcp_client.py`** - Comunicaci√≥n TCP
- Cliente TCP que conecta con el agente DQN en Jetson
- Protocolo: Env√≠o s√≠ncrono de estados y recepci√≥n de acciones
- Formato de datos: CSV para estados, enteros para acciones

#### Jetson Agent (C++/CUDA)

**`neural_network.cuh`** - Red Neuronal DQN
- Implementaci√≥n de red neuronal profunda en CUDA
- Arquitectura: Input(10) -> Dense(128,ReLU) -> Dense(128,ReLU) -> Dense(64,ReLU) -> Output(5)
- Kernels CUDA personalizados para forward/backward pass
- Gesti√≥n eficiente de memoria GPU
- Target Network para estabilidad

**`replay_buffer.hpp`** - Buffer de Experiencias
- Almacenar experiencias (s,a,r,s') para entrenamiento por lotes
- Capacidad: 100,000 experiencias con sobrescritura circular
- Muestreo: Selecci√≥n aleatoria de mini-lotes de 64 experiencias

**`dqn_agent.cuh`** - Agente DQN Principal
- Implementa el algoritmo completo de Deep Q-Network
- Pol√≠tica epsilon-greedy con decaimiento (1.0 -> 0.05)
- Entrenamiento online cada paso
- Soft update de target network (tau = 0.005)
- Guardado/carga de modelos entrenados
- Hiperpar√°metros optimizados: lr=0.0005, gamma=0.99, batch=64

**`tcp_server.hpp`** - Servidor TCP
- Servidor TCP que recibe estados y env√≠a acciones
- Parseo seguro con manejo de errores de formato
- Funciones de seguridad: safe_stof(), safe_stoi()

**`main.cu`** - Programa Principal
- Orquestaci√≥n del entrenamiento completo
- Inicializaci√≥n de CUDA y red neuronal
- Creaci√≥n de servidor TCP en puerto 5555
- Ciclo de entrenamiento epis√≥dico
- Guardado peri√≥dico de modelos
- Logging de recompensas, epsilon, loss durante entrenamiento

## Protocolo de Comunicaci√≥n TCP/IP

### Arquitectura de Red

- **Jetson Xavier**: Act√∫a como SERVIDOR (puerto 5555)
- **PC**: Act√∫a como CLIENTE (conecta al Jetson)
- **Protocolo**: S√≠ncrono, un mensaje por paso de simulaci√≥n

### Datos Enviados: PC -> Jetson (Estado del Robot)

El PC env√≠a el estado completo del robot en formato CSV con 10 valores:

```csv
x,y,theta,v,omega,d_front,d_left,d_right,dx_goal,dy_goal|done,reward,goal,collision
```

| Campo | Tipo | Rango | Descripci√≥n |
|-------|------|-------|-------------|
| `x` | float | [0.0, 3.0] | Posici√≥n X del robot (metros) |
| `y` | float | [0.0, 3.0] | Posici√≥n Y del robot (metros) |
| `theta` | float | [-pi, pi] | Orientaci√≥n del robot (radianes) |
| `v` | float | [0.0, 2.0] | Velocidad lineal actual (m/s) |
| `omega` | float | [-2.0, 2.0] | Velocidad angular actual (rad/s) |
| `d_front` | float | [0.0, 5.0] | Distancia sensor frontal (metros) |
| `d_left` | float | [0.0, 5.0] | Distancia sensor izquierdo (metros) |
| `d_right` | float | [0.0, 5.0] | Distancia sensor derecho (metros) |
| `dx_goal` | float | [-3.0, 3.0] | Componente X hacia objetivo |
| `dy_goal` | float | [-3.0, 3.0] | Componente Y hacia objetivo |

Ejemplo de mensaje real:
```
1.5,0.8,0.785,1.2,-0.3,2.1,1.8,3.2,-1.0,1.7|0,-0.1,0,0
```

### Datos Recibidos: Jetson -> PC (Acci√≥n Seleccionada)

El Jetson responde con un entero que representa la acci√≥n a ejecutar:

| Acci√≥n | Valor | Efecto en el Robot |
|--------|-------|-------------------|
| FORWARD | 0 | Acelerar hacia adelante (v += 0.5) |
| LEFT | 1 | Girar a la izquierda (omega += 1.0) |
| RIGHT | 2 | Girar a la derecha (omega -= 1.0) |
| BRAKE | 3 | Frenar (v *= 0.5, omega *= 0.5) |
| BACKWARD | 4 | Retroceder (v -= 0.3) |

### Procesamiento en Jetson Xavier

1. Recepci√≥n y Parseo de Estados
   - Separar por comas y convertir con safe_stof()
   - Validar rangos y detectar errores de formato
   - Retornar vector normalizado para la red neuronal

2. Inferencia DQN (Forward Pass)
   - Normalizaci√≥n: Estados se normalizan a rango [0,1]
   - GPU Transfer: Datos se copian a memoria GPU
   - Forward Pass: Red neuronal procesa entrada -> Q-values
   - Selecci√≥n de Acci√≥n: epsilon-greedy sobre Q-values m√°ximos
   - CPU Return: Acci√≥n seleccionada regresa a CPU

3. Entrenamiento DQN (Backward Pass)
   - Muestrear mini-lote del replay buffer (64 experiencias)
   - Calcular Q-targets usando target network
   - Forward pass en main network
   - Calcular loss MSE: (Q_pred - Q_target)^2
   - Backward pass y actualizaci√≥n de pesos (Adam optimizer)
   - Soft update de target network

## Algoritmo Deep Q-Network (DQN)

### Arquitectura de Red Neuronal

```
Entrada (10 dimensiones)
    |
Capa Densa 1: 10 -> 128 neuronas + ReLU
    |  
Capa Densa 2: 128 -> 128 neuronas + ReLU
    |
Capa Densa 3: 128 -> 64 neuronas + ReLU  
    |
Salida: 64 -> 5 Q-values (una por acci√≥n)
```

Par√°metros totales: ~35,000 pesos entrenables

### Funci√≥n de P√©rdida (Loss Function)

```
L(theta) = E[(Q_target - Q_pred)^2]

Donde:
Q_target = r + gamma * max_a' Q_target(s', a')
Q_pred = Q_main(s, a)
```

### Hiperpar√°metros

| Par√°metro | Valor | Descripci√≥n |
|-----------|-------|-------------|
| Learning Rate | 0.0005 | Tasa de aprendizaje |
| Gamma | 0.99 | Factor de descuento |
| Epsilon inicial | 1.0 | Exploraci√≥n inicial |
| Epsilon final | 0.05 | Exploraci√≥n m√≠nima |
| Epsilon decay | 0.9999 | Decaimiento de exploraci√≥n |
| Batch Size | 64 | Tama√±o del mini-lote |
| Replay Buffer | 100,000 | Capacidad del buffer |
| Tau | 0.005 | Factor de actualizaci√≥n de target network |
| Train Frequency | 1 | Entrenamiento en cada paso |

### Funci√≥n de Recompensa

```python
def calculate_reward(self):
    reward = 0.0
    
    # Objetivo alcanzado
    if self.check_goal_reached():
        return +100.0
    
    # Colisi√≥n
    if self.check_collision():
        return -100.0
    
    # Progreso hacia el objetivo
    dist_actual = np.linalg.norm(self.position - self.goal)
    if dist_actual < self.prev_distance:
        reward += 10.0 * (self.prev_distance - dist_actual)
    
    # Costo por tiempo
    reward -= 0.1
    
    # Penalizaci√≥n por acciones ineficientes
    if action == BRAKE:
        reward -= 0.2
    elif action == BACKWARD:
        reward -= 0.15
    
    # Bonificaci√≥n por proximidad al objetivo
    if dist_actual < 1.0:
        reward += 0.5
    elif dist_actual < 2.0:
        reward += 0.2
        
    # Penalizaci√≥n por proximidad a obst√°culos
    if min(d_front, d_left, d_right) < 0.3:
        reward -= 0.5
        
    return reward
```

### Proceso de Entrenamiento

#### Ciclo de Entrenamiento por Episodio:

1. Inicializaci√≥n
   - Robot en posici√≥n (0.1, 0.1)
   - Objetivo en (2.5, 2.5) 
   - Obst√°culos fijos en posiciones estrat√©gicas

2. Loop de Pasos (m√°ximo 150 pasos por episodio)
   - Recibir estado del PC
   - Seleccionar acci√≥n (epsilon-greedy)
   - Enviar acci√≥n al PC
   - Almacenar experiencia
   - Entrenar si hay suficientes experiencias

3. Actualizaci√≥n de Par√°metros
   - Decaimiento de epsilon
   - Soft update de target network
   - Guardado de modelo cada 100 episodios

## Configuraci√≥n y Ejecuci√≥n

### Configuraci√≥n de Red

#### Paso 1: Configurar IPs de los Dispositivos

En el Jetson Xavier (Servidor):
```bash
# Obtener IP del Jetson
ip addr show eth0        # Ethernet
# O para WiFi:
ip addr show wlan0

# Ejemplo de salida: inet 192.168.18.114/24
```

En el PC (Cliente):
```bash
# Verificar conectividad con el Jetson
ping 192.168.18.114     # Usar la IP real del Jetson

# Verificar puerto abierto
nc -zv 192.168.18.114 5555
```

#### Paso 2: Configurar Firewall (si es necesario)

En el Jetson Xavier:
```bash
# Permitir puerto 5555 para comunicaci√≥n TCP
sudo ufw allow 5555/tcp

# O desactivar firewall temporalmente
sudo ufw disable
```

### Instalaci√≥n y Compilaci√≥n

#### En el PC (Python 3.8+)
```bash
# Instalar dependencias del simulador
pip3 install numpy pygame

# Verificar instalaci√≥n
cd pc_simulator
python3 -c "import numpy, pygame; print('Dependencias OK')"
```

#### En el Jetson Xavier (CUDA 12.2+)
```bash
# Transferir c√≥digo al Jetson
scp -r jetson_agent/ usuario@192.168.18.114:~/DQN_JetsonXavier/

# Conectar al Jetson y compilar
ssh usuario@192.168.18.114
cd ~/DQN_JetsonXavier/jetson_agent

# Verificar CUDA disponible
nvidia-smi
nvcc --version

# Compilar el agente DQN
make clean && make

# Verificar compilaci√≥n exitosa
ls -la bin/dqn_agent
```

### Ejecuci√≥n del Sistema

#### Secuencia de Inicio (IMPORTANTE: Orden espec√≠fico)

1. Iniciar Servidor DQN en Jetson (PRIMERO):
```bash
# En terminal del Jetson Xavier
cd ~/DQN_JetsonXavier/jetson_agent
./bin/dqn_agent --port 5555 --episodes 500
```

2. Iniciar Cliente Simulador en PC (SEGUNDO):
```bash
# En terminal del PC
cd pc_simulator
python3 tcp_client.py --ip 192.168.18.114 --port 5555 --episodes 500 --visualize
```

### Par√°metros de Configuraci√≥n

#### Opciones del Simulador (PC)
| Par√°metro | Descripci√≥n | Valor Default |
|-----------|-------------|---------------|
| --ip | IP del Jetson Xavier | 127.0.0.1 |
| --port | Puerto TCP de comunicaci√≥n | 5555 |
| --episodes | N√∫mero total de episodios | 1000 |
| --visualize | Mostrar GUI en tiempo real | False |
| --render-every | Renderizar cada N episodios | 10 |
| --save-logs | Guardar m√©tricas en archivo | True |

#### Opciones del Agente DQN (Jetson)
| Par√°metro | Descripci√≥n | Valor Default |
|-----------|-------------|---------------|
| --port | Puerto TCP del servidor | 5555 |
| --episodes | Episodios m√°ximos (-1=infinito) | -1 |
| --model-path | Ruta del modelo DQN | models/dqn_model.bin |
| --load-model | Cargar modelo existente | False |
| --save-every | Guardar modelo cada N episodios | 100 |
| --device-id | ID del dispositivo CUDA | 0 |

### Monitoreo del Entrenamiento

#### En el Jetson (Logs de IA):
```
[EPISODE 0001] Reward: -85.4  | Loss: 2.45  | Epsilon: 0.999 | Steps: 150
[EPISODE 0050] Reward: -12.3  | Loss: 0.87  | Epsilon: 0.951 | Steps: 89
[EPISODE 0100] Reward: +45.7  | Loss: 0.34  | Epsilon: 0.905 | Steps: 67
[EPISODE 0200] Reward: +89.2  | Loss: 0.18  | Epsilon: 0.819 | Steps: 34
```

#### En el PC (Logs de Simulaci√≥n):
```
[SIM] Episodio 1/500 | Goal: NO | Colisi√≥n: SI | Pasos: 150 | R_total: -85.4
[SIM] Episodio 50/500 | Goal: NO | Colisi√≥n: SI | Pasos: 89 | R_total: -12.3  
[SIM] Episodio 100/500 | Goal: SI | Colisi√≥n: NO | Pasos: 67 | R_total: +45.7
[SIM] Episodio 200/500 | Goal: SI | Colisi√≥n: NO | Pasos: 34 | R_total: +89.2
```

## Requisitos del Sistema

### PC (Cliente - Simulador)
- CPU: Intel/AMD multi-core (4+ cores recomendado)
- RAM: 4 GB m√≠nimo, 8 GB recomendado  
- Python: 3.8+
- Dependencias: NumPy, Pygame
- Red: Conexi√≥n Ethernet/WiFi estable con Jetson

### Jetson Xavier (Servidor - Agente IA)
- GPU: 512 CUDA cores (Volta), 32 Tensor cores
- RAM: 16/32 GB
- CUDA: Compute Capability 7.2 (sm_72)
- Storage: 10 GB disponible para modelos y logs
- Red: Ethernet preferido para estabilidad

### Configuraci√≥n CUDA

El Makefile incluye optimizaciones espec√≠ficas por arquitectura:

```makefile
# Jetson Nano (Maxwell): sm_53
# NVCC_FLAGS += -arch=sm_53

# Jetson TX2 (Pascal): sm_62  
# NVCC_FLAGS += -arch=sm_62

# Jetson Xavier (Volta): sm_72 [ACTUAL]
NVCC_FLAGS += -arch=sm_72

# Jetson Orin (Ampere): sm_87
# NVCC_FLAGS += -arch=sm_87
```

